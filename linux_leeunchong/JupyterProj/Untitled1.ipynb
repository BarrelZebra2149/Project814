{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac34e1a-1ab4-4d6f-9dc9-923cb98ff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Device & constants\n",
    "# ────────────────────────────────────────────────\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using PyTorch device: {DEVICE}\")\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "IND_ROWS = 8\n",
    "IND_COLS = 14\n",
    "\n",
    "# 8-directional deltas (as tensor on device)\n",
    "DELTAS = torch.tensor([\n",
    "    [-1, -1], [-1, 0], [-1, 1],\n",
    "    [0, -1],          [0, 1],\n",
    "    [1, -1], [1, 0], [1, 1]\n",
    "], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Helper: precompute positions per digit on GPU\n",
    "# ────────────────────────────────────────────────\n",
    "def positions_per_digit(grid_t):\n",
    "    pos = {}\n",
    "    for d in range(10):\n",
    "        mask = (grid_t == d)\n",
    "        rs, cs = torch.nonzero(mask, as_tuple=True)\n",
    "        if len(rs) > 0:\n",
    "            pos[d] = torch.stack([rs, cs], dim=1)  # [N_positions, 2]\n",
    "    return pos\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Batched path check: GPU-accelerated multi-source BFS with visited cells\n",
    "# ────────────────────────────────────────────────\n",
    "def batch_has_path_torch(grid_np, numbers, precomputed_pos_dict=None):\n",
    "    \"\"\"\n",
    "    grid_np: np.ndarray (8,14) int32/uint8\n",
    "    numbers: list[int] or np.array\n",
    "    precomputed_pos_dict: optional dict[int → torch.Tensor] for reuse (faster in GA)\n",
    "    Returns: np.ndarray[int] (len(numbers),)  1 if path exists, 0 otherwise\n",
    "    \"\"\"\n",
    "    if DEVICE == 'cpu':\n",
    "        print(\"Warning: Falling back to CPU (slow)\")\n",
    "        return np.zeros(len(numbers), dtype=int)  # placeholder; replace with CPU DFS if needed\n",
    "\n",
    "    grid_t = torch.from_numpy(grid_np.astype(np.int32)).to(DEVICE)\n",
    "\n",
    "    # Use precomputed if provided (e.g., in GA per individual)\n",
    "    if precomputed_pos_dict is not None:\n",
    "        pos_dict = precomputed_pos_dict\n",
    "    else:\n",
    "        pos_dict = positions_per_digit(grid_t)\n",
    "\n",
    "    results = torch.zeros(len(numbers), dtype=torch.int32, device=DEVICE)\n",
    "\n",
    "    for i, n in enumerate(numbers):\n",
    "        if n <= 0:\n",
    "            results[i] = 1 if (n == 0 and 0 in pos_dict) else 0\n",
    "            continue\n",
    "\n",
    "        str_n = str(n)\n",
    "        digits = torch.tensor([int(d) for d in str_n], device=DEVICE)\n",
    "        len_path = len(digits)\n",
    "\n",
    "        first_d = digits[0].item()\n",
    "        if first_d not in pos_dict or len(pos_dict[first_d]) == 0:\n",
    "            continue\n",
    "\n",
    "        # Single digit: yes if exists\n",
    "        if len_path == 1:\n",
    "            results[i] = 1\n",
    "            continue\n",
    "\n",
    "        # Multi-digit: BFS level-by-level (per digit)\n",
    "        visited = torch.zeros_like(grid_t, dtype=torch.bool)\n",
    "        current_front = pos_dict[first_d].clone()  # All starting positions [N_starts, 2]\n",
    "        visited[current_front[:, 0], current_front[:, 1]] = True\n",
    "\n",
    "        found = False\n",
    "        for depth in range(1, len_path):\n",
    "            next_d = digits[depth].item()\n",
    "            if next_d not in pos_dict:\n",
    "                break\n",
    "\n",
    "            # Vectorized expansion: broadcast current_front to 8 directions\n",
    "            r_offsets = current_front[:, 0][:, None] + DELTAS[:, 0][None, :]\n",
    "            c_offsets = current_front[:, 1][:, None] + DELTAS[:, 1][None, :]\n",
    "            expanded_r = r_offsets.flatten()\n",
    "            expanded_c = c_offsets.flatten()\n",
    "\n",
    "            # Filter valid positions\n",
    "            valid_mask = (\n",
    "                (expanded_r >= 0) & (expanded_r < IND_ROWS) &\n",
    "                (expanded_c >= 0) & (expanded_c < IND_COLS)\n",
    "            )\n",
    "            nr = expanded_r[valid_mask]\n",
    "            nc = expanded_c[valid_mask]\n",
    "\n",
    "            if len(nr) == 0:\n",
    "                break\n",
    "\n",
    "            # Check if neighbor matches next digit AND not visited\n",
    "            is_next = (grid_t[nr, nc] == next_d)\n",
    "            not_visited = ~visited[nr, nc]\n",
    "            new_front_mask = is_next & not_visited\n",
    "\n",
    "            if not new_front_mask.any():\n",
    "                break  # No way to proceed\n",
    "\n",
    "            # Update front to new positions\n",
    "            new_r = nr[new_front_mask]\n",
    "            new_c = nc[new_front_mask]\n",
    "            current_front = torch.stack([new_r, new_c], dim=1)\n",
    "\n",
    "            # Mark as visited (prevent reuse)\n",
    "            visited[new_r, new_c] = True\n",
    "\n",
    "            # If we reached the last digit level and have positions, path exists\n",
    "            if depth == len_path - 1:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if found:\n",
    "            results[i] = 1\n",
    "\n",
    "    return results.cpu().numpy()\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Test function (adapted: precompute pos_dict once, test reverse numbers too)\n",
    "# ────────────────────────────────────────────────\n",
    "def run_test():\n",
    "    np.random.seed(42)\n",
    "    grid_np = np.random.randint(0, 10, size=(IND_ROWS, IND_COLS), dtype=np.int32)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEST GRID (8x14):\")\n",
    "    print(\"=\" * 70)\n",
    "    for row in grid_np:\n",
    "        print(' '.join(f\"{x:1d}\" for x in row))\n",
    "\n",
    "    # Precompute positions once for speed (useful in GA too)\n",
    "    grid_t = torch.from_numpy(grid_np.astype(np.int32)).to(DEVICE)\n",
    "    pos_dict = positions_per_digit(grid_t)\n",
    "\n",
    "    test_numbers = [1, 12, 123, 1234, 5678, 9999, 42, 100, 314159]\n",
    "    # Add some reverses for testing reverse optimization\n",
    "    test_numbers += [21, 321, 8765, 9999]  # e.g., reverse of 12, 123, 5678\n",
    "\n",
    "    print(\"\\nTesting individual numbers (including reverses):\")\n",
    "    results = batch_has_path_torch(grid_np, test_numbers, precomputed_pos_dict=pos_dict)\n",
    "    for n, res in zip(test_numbers, results):\n",
    "        print(f\"{n:7d} → {'YES' if res == 1 else 'no'}\")\n",
    "\n",
    "    # Performance test: larger batch\n",
    "    print(\"\\nPerformance test:\")\n",
    "    consecutive_nums = list(range(1, 1000))  # larger for meaningful timing\n",
    "    random_4digit = [random.randint(1000, 9999) for _ in range(1000)]\n",
    "    all_nums = consecutive_nums + random_4digit\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    _ = batch_has_path_torch(grid_np, all_nums, precomputed_pos_dict=pos_dict)\n",
    "    t_gpu = time.perf_counter() - t0\n",
    "    print(f\"Time for {len(all_nums)} numbers (GPU): {t_gpu:.4f} seconds\")\n",
    "    print(f\"Approx. checks/sec: {len(all_nums) / t_gpu:.0f}\" if t_gpu > 0 else \"N/A\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting full test...\")\n",
    "    run_test()\n",
    "    print(\"\\nTest complete. Ready for GA integration (use precomputed_pos_dict per individual).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e973b-a534-41c4-b2e1-c62506af99ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
